{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1549d581",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3a163",
   "metadata": {},
   "source": [
    "a dataset that includes listings for various areas in Montreal in 2019. It includes detailed information for each listing, such as thumbnails and detailed summarization about each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290f4f7",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014ae0a",
   "metadata": {},
   "source": [
    "predict the listing price based on the listing characteristics, the prices are in categories 0,1,2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160de622",
   "metadata": {},
   "source": [
    "# What data mining function is required?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ecd832",
   "metadata": {},
   "source": [
    "multi-objective (multi-task) multi-modality classification using neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec2d5e7",
   "metadata": {},
   "source": [
    "# What could be the challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa89639",
   "metadata": {},
   "source": [
    "Developing a successful solution to our problem , complex data,datasets can include complex data elements ,another thing is that we have to make sure that our algorithm must be efficient and scalable to extract information from the big data and we should have enough knowledge and experience in order to use them if we needed to improve our algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd195c9c",
   "metadata": {},
   "source": [
    "# What is the impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844af6c2",
   "metadata": {},
   "source": [
    "Our model predictions is going to implement a listing price based on the listing characteristics which will help people ask for suitable prices on airbnb insteaf of checking prices of similar posts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0b867",
   "metadata": {},
   "source": [
    "# What is an ideal solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2891805",
   "metadata": {},
   "source": [
    " An ideal solution in my opinion will be measured in terms of metrics and performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b945f1",
   "metadata": {},
   "source": [
    "# What is the experimental protocol used and how was it carried out?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a48cd0c",
   "metadata": {},
   "source": [
    "For image data is tackled using convolutional neural network and the text data is processed using embedding and GRU/LSTM layers.\n",
    "\n",
    "When it comes to preprocessing,  I preprocessed the images by resizing them and converting them to 'LA' , created an array of each images and then returned a new array for 64, 64, 2)  shape, filled with zeroes, loaded images which are in our trainning dataset, identified them using the column 'image' \n",
    "\n",
    "for the 'summary' column , I converted some of the non-string cells to string cells        \n",
    "\n",
    "And I used neural networks in my model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc754c7",
   "metadata": {},
   "source": [
    "# How did we tune hyper-parameters in the template?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec86c6",
   "metadata": {},
   "source": [
    "To evaluate hyperparameter combinations, the search function accepts as input training data and a validation split. In random search and Bayesian Optimization, the epochs parameter is used to specify the number of training epochs for each hyperparameter combination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07d7aec",
   "metadata": {},
   "source": [
    "# What is the search space and what is the criteria to determine good/bad hyper-parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e434ae",
   "metadata": {},
   "source": [
    "Search space. This component describes the set of possible neural network architectures to consider. These search spaces are designed specific to the application, e.g., a space of convolutional networks for computer vision tasks or a space of recurrent networks for language modeling tasks.\n",
    "\n",
    "There are basically four methods to determine hyperparameters:\n",
    "\n",
    "Manual Search: Using knowledge you have about the problem guess parameters and observe the result. \n",
    "\n",
    "\n",
    "Grid Search:Determine ranges for the hyperparameters based on your knowledge of the problem. Then choose a few points from those ranges that are usually evenly distributed. Train your network with every possible parameter combination and pick the one that works best. Alternatively, you can refine your search to a more specific domain centred on the best-performing characteristics.\n",
    "\n",
    "\n",
    "Random Search:To identify ranges for the hyperparameters, you leverage knowledge of the problem in the same way that grid search does. Instead of selecting values from such ranges in a systematic manner, you choose them at random. Use what you learn to reduce your search or repeat this procedure until you find settings that work well. Dr. Bengio suggests this as the baseline approach against which all other methods should be assessed in his paper Random Search for Hyper-Parameter Optimization, and shows that it works better than the others.\n",
    "\n",
    "Bayesian Optimization:More recent work has focused on improving on these other approaches by deciding how to alter the hyper parameters for the next experiment based on the knowledge acquired from any specific experiment. Adams et alPractical .'s Bayesian Optimization of Machine Learning Algorithms is an example of this type of work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67b6ac",
   "metadata": {},
   "source": [
    "# Is fully-connected model a good one for sequential data? Why? How about for image data? Is it good? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad70792",
   "metadata": {},
   "source": [
    "Deep learning's workhorses, fully connected networks, are employed in thousands of applications. Fully connected networks have the virtue of being \"structure agnostic.\" That is, no specific assumptions about the input are required (for example, that the input consists of images or videos).\n",
    "\n",
    "Fully Connected Layer is simply, feed forward neural networks.Fully Connected Layers are the network's final layers. The output from the final Pooling or Convolutional Layer, which is flattened and then fed into the fully connected layer, is the input to the fully connected layer. so it's super good for both sequential data & image data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d901a1",
   "metadata": {},
   "source": [
    "# What is gradient vanishing and gradient explosion, and how GRU/LSTM tries to mitigate this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2815ab",
   "metadata": {},
   "source": [
    "n derivatives will be multiplied together in a network of n hidden layers. If the derivatives are significant, the gradient will grow exponentially as we propagate down the model until it explodes, which is referred to as the exploding gradient problem. If the derivatives are tiny, on the other hand, the gradient will fall exponentially as we propagate through the model until it vanishes, which is known as the vanishing gradient problem.\n",
    "\n",
    "The Sigmoid function saturates at 0 or 1 with a derivative very close to zero for bigger inputs (negative or positive), as shown in the graph above. As a result, when the backpropagation process kicks in, there are almost no gradients to propagate backward in the network, and any residual gradients there are continue to dilute as the programme continues through the top layers. As a result, the bottom layers are left with nothing.\n",
    "\n",
    "GRU/LSTM:\n",
    "\n",
    "As a solution to short-term memory, LSTMs and GRUs were developed. They have inbuilt devices known as gates that can control the flow of data.\n",
    "\n",
    "These gates can figure out which data in a sequence should be kept and which should be discarded. It can then transfer important information down the long chain of sequences to make predictions as a result of this. These two networks are responsible for nearly all state-of-the-art recurrent neural network findings. Voice recognition, speech synthesis, and text production all use LSTMs and GRUs. You can even use them to create video captioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac8c98",
   "metadata": {},
   "source": [
    "# What is multi-objective/multi-task learning? What is multi-modality learning? How do you use them in this assignment?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b69547",
   "metadata": {},
   "source": [
    "Multi-task learning: is a machine learning approach in which we try to learn multiple tasks simultaneously, optimizing multiple loss functions at once. Rather than training independent models for each task, we allow a single model to learn to complete all of the tasks at once. In this process, the model uses all of the available data across the different tasks to learn generalized representations of the data that are useful in multiple contexts.\n",
    "\n",
    "Multi-modality learning: Multimodal learning involves relating information from multiple sources. In our case, we are trying to use the textual data and the image data together to get the required outcome. We make use of the the Multiclass Classification.\n",
    "\n",
    "We used them in the assignment by predicting both price & type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f26ca",
   "metadata": {},
   "source": [
    "# What is the difference among xgboost, lightgbm and catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f384c82",
   "metadata": {},
   "source": [
    "Catboost cultivates a well-balanced tree. Leaf-wise (best-first) tree growth is used by LightGBM. It selects to grow the leaf that causes the least amount of loss, allowing the tree to develop in an unbalanced state. When data is small, overfitting might occur because it grows leaf-wise rather than level-wise.\n",
    "\n",
    "XGBoost was originally produced by University of Washington researchers and is maintained by open-source contributors. Similar to LightGBM, XGBoost uses the gradients of different cuts to select the next cut, but XGBoost also uses the hessian, or second derivative, in its ranking of cuts. Computing this next derivative comes at a slight cost, but it also allows a greater estimation of the cut to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e54643",
   "metadata": {},
   "source": [
    "# Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c830e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first I started importing some necessary libraries\n",
    "import pandas as pd #I imported pandas cause Using it\n",
    "#make it way easier when it comes to  processing, analysis and manipulation of data\n",
    "from tqdm.notebook import tqdm # tqdm derives from the Arabic word taqaddum (تقدّم) which can mean “progress,” Instantly make your loops show a smart progress meter\n",
    "from PIL import Image # library for image processing.\n",
    "import os #to be able to use operating system dependant functions\n",
    "import numpy as np #to deal with arrays\n",
    "from ast import literal_eval #expands to Abstract Syntax Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5160b663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>image</th>\n",
       "      <th>type</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
       "      <td>img_train/0.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Located in one of the most vibrant and accessi...</td>\n",
       "      <td>img_train/1.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logement coquet et douillet à 10 minutes du ce...</td>\n",
       "      <td>img_train/2.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
       "      <td>img_train/3.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Très grand appartement ''rustique'' et très ag...</td>\n",
       "      <td>img_train/4.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7622</th>\n",
       "      <td>Un grand logement 4 et 1/2, tout inclut, bien ...</td>\n",
       "      <td>img_train/7626.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>Magnificent condo directly on the river. You w...</td>\n",
       "      <td>img_train/7627.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7624</th>\n",
       "      <td>This apartment is perfect for anyone visiting ...</td>\n",
       "      <td>img_train/7628.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7625</th>\n",
       "      <td>It is a cozy ,clean ,and comfortable apartment...</td>\n",
       "      <td>img_train/7629.jpg</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7626</th>\n",
       "      <td>Modern country style (newly-renovated); open c...</td>\n",
       "      <td>img_train/7630.jpg</td>\n",
       "      <td>House</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7627 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                summary               image  \\\n",
       "0     Spacious, sunny and cozy modern apartment in t...     img_train/0.jpg   \n",
       "1     Located in one of the most vibrant and accessi...     img_train/1.jpg   \n",
       "2     Logement coquet et douillet à 10 minutes du ce...     img_train/2.jpg   \n",
       "3     Beautiful and spacious (1076 sc ft, / 100 mc) ...     img_train/3.jpg   \n",
       "4     Très grand appartement ''rustique'' et très ag...     img_train/4.jpg   \n",
       "...                                                 ...                 ...   \n",
       "7622  Un grand logement 4 et 1/2, tout inclut, bien ...  img_train/7626.jpg   \n",
       "7623  Magnificent condo directly on the river. You w...  img_train/7627.jpg   \n",
       "7624  This apartment is perfect for anyone visiting ...  img_train/7628.jpg   \n",
       "7625  It is a cozy ,clean ,and comfortable apartment...  img_train/7629.jpg   \n",
       "7626  Modern country style (newly-renovated); open c...  img_train/7630.jpg   \n",
       "\n",
       "           type  price  \n",
       "0     Apartment      1  \n",
       "1     Apartment      0  \n",
       "2     Apartment      1  \n",
       "3     Apartment      1  \n",
       "4     Apartment      0  \n",
       "...         ...    ...  \n",
       "7622  Apartment      0  \n",
       "7623  Apartment      2  \n",
       "7624  Apartment      1  \n",
       "7625  Apartment      0  \n",
       "7626      House      1  \n",
       "\n",
       "[7627 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading training dataset\n",
    "tr=pd.read_csv('train_xy.csv')\n",
    "tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b79586db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Charming warm house is ready to host you here ...</td>\n",
       "      <td>img_test/0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>La chambre est spacieuse et lumineuse, dans un...</td>\n",
       "      <td>img_test/1.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Grande chambre confortable située au sous-sol ...</td>\n",
       "      <td>img_test/2.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Près d’un Métro, ligne orange. 10 minutes à pi...</td>\n",
       "      <td>img_test/3.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Very bright appartment and very cosy. 2 separa...</td>\n",
       "      <td>img_test/4.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7626</th>\n",
       "      <td>Large, fully-furnished flat with brick walls a...</td>\n",
       "      <td>img_test/7627.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7627</th>\n",
       "      <td>Logement situé dans le haut d’un duplex. Vivez...</td>\n",
       "      <td>img_test/7628.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7628</th>\n",
       "      <td>My place is close to parks, . My place is good...</td>\n",
       "      <td>img_test/7629.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7629</th>\n",
       "      <td>*** For security reasons, I will prioritize gu...</td>\n",
       "      <td>img_test/7630.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7630</th>\n",
       "      <td>Stay in an amazing area of Montreal! 5-7 min f...</td>\n",
       "      <td>img_test/7631.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7360 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                summary              image\n",
       "id                                                                        \n",
       "0     Charming warm house is ready to host you here ...     img_test/0.jpg\n",
       "1     La chambre est spacieuse et lumineuse, dans un...     img_test/1.jpg\n",
       "2     Grande chambre confortable située au sous-sol ...     img_test/2.jpg\n",
       "3     Près d’un Métro, ligne orange. 10 minutes à pi...     img_test/3.jpg\n",
       "4     Very bright appartment and very cosy. 2 separa...     img_test/4.jpg\n",
       "...                                                 ...                ...\n",
       "7626  Large, fully-furnished flat with brick walls a...  img_test/7627.jpg\n",
       "7627  Logement situé dans le haut d’un duplex. Vivez...  img_test/7628.jpg\n",
       "7628  My place is close to parks, . My place is good...  img_test/7629.jpg\n",
       "7629  *** For security reasons, I will prioritize gu...  img_test/7630.jpg\n",
       "7630  Stay in an amazing area of Montreal! 5-7 min f...  img_test/7631.jpg\n",
       "\n",
       "[7360 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading test dataset\n",
    "ts=pd.read_csv('test_x.csv', index_col = 'id')\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3444cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7627, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.shape #displaying shape of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2305cd35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7360, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.shape #displaying shape of testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62b36959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7627.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.442769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.611946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             price\n",
       "count  7627.000000\n",
       "mean      0.442769\n",
       "std       0.611946\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       1.000000\n",
       "max       2.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.describe() #some information about the training dataset such as standard deviation,mean...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f508a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7360</td>\n",
       "      <td>7360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>6903</td>\n",
       "      <td>7360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Featured in The New York Times, The Wall Stree...</td>\n",
       "      <td>img_test/0.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  summary           image\n",
       "count                                                7360            7360\n",
       "unique                                               6903            7360\n",
       "top     Featured in The New York Times, The Wall Stree...  img_test/0.jpg\n",
       "freq                                                   93               1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts.describe()#some information about testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b015508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999f373216194aa693669873c98b7b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preprocess image data\n",
    "\n",
    "def load_image(file):\n",
    "    try:\n",
    "        # open image file\n",
    "        image = Image.open(\n",
    "            file\n",
    "        # image is converted to 'LA' and is also being resized\n",
    "        ).convert('LA').resize((64, 64))\n",
    "        # creating an array of each images\n",
    "        arr = np.array(image)\n",
    "    except:\n",
    "        # returns a new array for given shape below, filled with zeroes\n",
    "        arr = np.zeros((64, 64, 2))\n",
    "    return arr\n",
    "\n",
    "\n",
    "# loading images:\n",
    "# loads images which are in our trainning dataset, identifies them using the column 'image'.\n",
    "from tqdm.notebook import tqdm\n",
    "# Tqdm is a Python library used to display smart progress bars that show the progress of your Python code execution.\n",
    "x_image = np.array([load_image(i) for i in tqdm(tr.image)])\n",
    "\n",
    "#  convert some of the non-string cell to string\n",
    "x_text = tr.summary.astype('str')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f38f95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels:\n",
    "# target column which is price \n",
    "y_price = tr.price\n",
    "#and then encoding our (type) column\n",
    "y_type = tr.type.astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec5ef902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price category number of unique values and unique values are : 3 [1 0 2]\n",
      "type category number of unique values and unique values are : 24 [ 1 17 22 10 18 20  5  2  8  4 23 13 15 16 14 11 19  0 21  3  6 12  7  9]\n"
     ]
    }
   ],
   "source": [
    "# calculate the total number of unique labels.\n",
    "len_price = len(y_price.unique())\n",
    "len_type = len(y_type.unique())\n",
    "print('price category number of unique values and unique values are :', len_price, y_price.unique()) #printing for price label\n",
    "print('type category number of unique values and unique values are :', len_type, y_type.unique())#printing for type label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "745e6ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into 70% ,30%: (training &validate=ion)\n",
    "from sklearn.model_selection import train_test_split # function used for data splitting\n",
    "\n",
    "x_tr_image, x_vl_image, x_tr_text, x_vl_text, y_tr_price, y_vl_price, y_tr_type, y_vl_type = train_test_split(\n",
    "    x_image, \n",
    "    x_text,\n",
    "    y_price,\n",
    "    y_type,\n",
    "    test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d2213ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5338, 100)\n",
      "(2289, 100)\n"
     ]
    }
   ],
   "source": [
    "# maximum number of words from the resulting tokenized data which are to be used\n",
    "# preprocess text data\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # to divide strings into lists of substrings\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #Pad sequences to  same length.\n",
    "from pprint import pprint #pretty-print Python data structures in a form which can be used as input to the interpreter \n",
    "vocab_size = 40000 #assigning vocab size\n",
    "max_len = 100 # and maximum length\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "# upadting the internal vocalulary based on the list of text, so it creates the vocabulary\n",
    "# index based on word frequnecy so every word gets a unique interger value so lower integers \n",
    "# mean more frequent word.\n",
    "tokenizer.fit_on_texts(x_tr_text)\n",
    "\n",
    "\n",
    "def _preprocess(list_of_text):\n",
    "    #padding 0 in the beggining of each sequence until they have the same length as\n",
    "    # the longest sequence. \n",
    "    return pad_sequences(\n",
    "        # texts_to sequence transforms every text in texts to a sequence of integers from dictionary\n",
    "        tokenizer.texts_to_sequences(list_of_text),\n",
    "        # takes in the pre-defined input (100) as maximum length of all sequences.\n",
    "        maxlen=max_len,\n",
    "        # does padding after each sequence\n",
    "        padding='post',\n",
    "    )\n",
    "    \n",
    "\n",
    "# padding is done inside: \n",
    "x_tr_text_id = _preprocess(x_tr_text)\n",
    "x_vl_text_id = _preprocess(x_vl_text)\n",
    "\n",
    "print(x_tr_text_id.shape) #printing for training \n",
    "print(x_vl_text_id.shape) #printing for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04dc44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras #Keras is a library that will help us make neural networks.\n",
    "import tensorflow as tf #open-source library for numerical computation\n",
    "\n",
    "# defines an input layer ,Input() is used to instantiate a Keras tensor, (instantiate a Keras tensor object) and allows for building a model.\n",
    "in_text = keras.Input(batch_shape=(None, max_len))\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06d3a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_words is  the maximum number of words to keep, based on word frequency.\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "#tf.reduce_mean() computes the mean of elements across dimensions of a tensor..\n",
    "averaged = tf.reduce_mean(embedded, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e357a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 49, 49, 32)   16416       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 100, 100)     4000000     ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 3, 3, 32)     0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " tf.math.reduce_mean (TFOpLambd  (None, 100)         0           ['embedding[0][0]']              \n",
      " a)                                                                                               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 288)          0           ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " tf.concat (TFOpLambda)         (None, 388)          0           ['tf.math.reduce_mean[0][0]',    \n",
      "                                                                  'flatten[0][0]']                \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            1167        ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      " type (Dense)                   (None, 24)           9336        ['tf.concat[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,026,919\n",
      "Trainable params: 4,026,919\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#image part\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "\n",
    "#Image inputs. Covering at least a Conv2d layer:\n",
    "# first convolution layer\n",
    "# the first parameter is the filter indicating the dimensionality of the output space.\n",
    "# second oe is the kernel_size it tells the height and width of the filter.\n",
    "cov = Conv2D(32, (16, 16))(in_image)\n",
    "# Max pooling layer to downsample\n",
    "pl = MaxPool2D((16, 16))(cov)\n",
    "# and flattening \n",
    "flattened = Flatten()(pl)\n",
    "\n",
    "\n",
    "# fusion - combining both:\n",
    "# concatenates tensors along one dimension (axis)\n",
    "fused = tf.concat([averaged, flattened], axis=-1)\n",
    "# multi-task learning (each is a multi-class classification)\n",
    "# one dense layer for each task\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
    "\n",
    "# define model input/output using keys.\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': in_text, #the inputs\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={ #to be predicted/outputs\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam #as it makes use of a gradient average that decays exponentially over time.\n",
    "\n",
    "# compile model with optimizer, loss values for each task, loss \n",
    "# weights for each task.\n",
    "model.compile(\n",
    "    optimizer=Adam(), # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.summary() #printing the summarzation of our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c66e56d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "234/234 [==============================] - 102s 406ms/step - loss: 32.3963 - price_loss: 28.0442 - type_loss: 36.7484 - price_sparse_categorical_accuracy: 0.4839 - type_sparse_categorical_accuracy: 0.5699 - val_loss: 13.1139 - val_price_loss: 9.1425 - val_type_loss: 17.0853 - val_price_sparse_categorical_accuracy: 0.6030 - val_type_sparse_categorical_accuracy: 0.7422\n",
      "Epoch 2/5\n",
      "234/234 [==============================] - 98s 417ms/step - loss: 10.6004 - price_loss: 8.3771 - type_loss: 12.8238 - price_sparse_categorical_accuracy: 0.5158 - type_sparse_categorical_accuracy: 0.5808 - val_loss: 10.5855 - val_price_loss: 12.2827 - val_type_loss: 8.8883 - val_price_sparse_categorical_accuracy: 0.6136 - val_type_sparse_categorical_accuracy: 0.5481\n",
      "Epoch 3/5\n",
      "234/234 [==============================] - 100s 429ms/step - loss: 7.4886 - price_loss: 5.3427 - type_loss: 9.6346 - price_sparse_categorical_accuracy: 0.5161 - type_sparse_categorical_accuracy: 0.5913 - val_loss: 5.0623 - val_price_loss: 3.7701 - val_type_loss: 6.3545 - val_price_sparse_categorical_accuracy: 0.5568 - val_type_sparse_categorical_accuracy: 0.6642\n",
      "Epoch 4/5\n",
      "234/234 [==============================] - 95s 406ms/step - loss: 4.4767 - price_loss: 3.5128 - type_loss: 5.4406 - price_sparse_categorical_accuracy: 0.5418 - type_sparse_categorical_accuracy: 0.6095 - val_loss: 4.8309 - val_price_loss: 4.2800 - val_type_loss: 5.3819 - val_price_sparse_categorical_accuracy: 0.4295 - val_type_sparse_categorical_accuracy: 0.5905\n",
      "Epoch 5/5\n",
      "234/234 [==============================] - 94s 401ms/step - loss: 4.1031 - price_loss: 3.0741 - type_loss: 5.1322 - price_sparse_categorical_accuracy: 0.5562 - type_sparse_categorical_accuracy: 0.6028 - val_loss: 4.5253 - val_price_loss: 3.4414 - val_type_loss: 5.6092 - val_price_sparse_categorical_accuracy: 0.5749 - val_type_sparse_categorical_accuracy: 0.5468\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( #fiting model\n",
    "    x={ #inputs\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={ #outputs\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=5, #num of ephocs ( number of passes through the training set) \n",
    "    batch_size=16, #numb of samples that will pass to network at a time\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
    "    ],\n",
    "    verbose=1 #shows progress and a line per each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "549aa808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4480d9e1438492d838154d446c1d539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_summary = _preprocess(ts.summary.astype(str)) \n",
    "x_test_image = np.array([load_image(i) for i in tqdm(ts.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70eda497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9939835e-01 6.0169719e-04 6.7562768e-11]\n",
      " [9.9987388e-01 1.2130430e-04 4.7702947e-06]\n",
      " [9.9950373e-01 4.9626583e-04 9.8856772e-09]\n",
      " ...\n",
      " [9.9997127e-01 1.8856001e-05 9.8910423e-06]\n",
      " [9.9999487e-01 5.1822353e-06 6.5851101e-13]\n",
      " [9.9875045e-01 1.2495047e-03 1.7307313e-09]]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict( #predicting results\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "price_predicted = y_predict['price']\n",
    "print(price_predicted) #predicting price\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame( #generating submission file\n",
    "    {'id': ts.index,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e3cf61",
   "metadata": {},
   "source": [
    "# Trial One\n",
    "\n",
    "Covering a Conv2d layer for Image inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df5067",
   "metadata": {},
   "source": [
    "# Expectaion\n",
    "\n",
    "I expect to get a high accuracy on kaggle after using conv2d layer for image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7ce91a",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "After running my code, my accuracy on kaggle was 0.51793"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e94b87",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "I will use LSTM layer for text inputs and see how my accuracy will get affected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968112d0",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "699df250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 100, 100)     4000000     ['input_5[0][0]']                \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 100, 124)     111600      ['embedding_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 49, 49, 32)   16416       ['input_6[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 100, 64)      48384       ['lstm_3[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 3, 3, 32)    0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)                  (None, 32)           12416       ['lstm_4[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 288)          0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " tf.concat_2 (TFOpLambda)       (None, 320)          0           ['lstm_5[0][0]',                 \n",
      "                                                                  'flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            963         ['tf.concat_2[0][0]']            \n",
      "                                                                                                  \n",
      " type (Dense)                   (None, 24)           7704        ['tf.concat_2[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,197,483\n",
      "Trainable params: 4,197,483\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "# Long Short Term Memory networks – usually just called “LSTMs”\n",
    "# It can handle not only individual data points like photos, but also speech or video\n",
    "\n",
    "from tensorflow import keras #Keras is a library that will help us make neural networks.\n",
    "import tensorflow as tf #open-source library for numerical computation\n",
    "\n",
    "\n",
    "# defines an input layer ,Input() is used to instantiate a Keras tensor, (instantiate a Keras tensor object) and allows for building a model\n",
    "in_text = keras.Input(batch_shape=(None, max_len))\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
    "\n",
    "# activation determines the output of that node given an input or group of inputs.\n",
    "# recurrent_activation: Activation function to use for the recurrent step\n",
    "# dropout: to avoid overfitting in training\n",
    "# rcurrent_dropout: regularization method for recurrent neural networks.\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "lstm = keras.layers.LSTM(124, \n",
    "                         activation=\"tanh\",\n",
    "                         recurrent_activation=\"sigmoid\",\n",
    "                         return_sequences=True,\n",
    "                         dropout=0.1,\n",
    "                         recurrent_dropout=0.1)(embedded)\n",
    "lstm2 = keras.layers.LSTM(64, \n",
    "                         activation=\"tanh\",\n",
    "                         recurrent_activation=\"sigmoid\",\n",
    "                         return_sequences=True,\n",
    "                         dropout=0.1,\n",
    "                         recurrent_dropout=0.1)(lstm)\n",
    "lstm3 = keras.layers.LSTM(32, \n",
    "                         activation=\"tanh\",\n",
    "                         recurrent_activation=\"sigmoid\",\n",
    "                         return_sequences=False,\n",
    "                         dropout=0.1,\n",
    "                         recurrent_dropout=0.1)(lstm2)\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "\n",
    "\n",
    "\n",
    "# image part\n",
    "# simple conv2d\n",
    "# first convolution layer\n",
    "# the first parameter is the filter indicating the dimensionality of the output space.\n",
    "# second oe is the kernel_size it tells the height and width of the filter.\n",
    "cov = Conv2D(32, (16, 16))(in_image)\n",
    "pl = MaxPool2D((16, 16))(cov)\n",
    "flattened = Flatten()(pl)\n",
    "\n",
    "\n",
    "# fusion - combining both:\n",
    "# concatenates tensors along one dimension (axis)\n",
    "fused = tf.concat([lstm3, flattened], axis=-1)\n",
    "\n",
    "# multi-task learning (each is a multi-class classification)\n",
    "# one dense layer for each task\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
    "\n",
    "# define model input/output using keys.\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': in_text,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={ #to be predicted\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "# compile model with optimizer, loss values for each task, loss \n",
    "# weights for each task.\n",
    "from tensorflow.keras.optimizers import Adam #optimizer\n",
    "\n",
    "model.compile( #compiling the model\n",
    "    optimizer=Adam(), # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "model.summary() #printing summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4b91482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "234/234 [==============================] - 330s 1s/step - loss: 35.1297 - price_loss: 34.6147 - type_loss: 35.6446 - price_sparse_categorical_accuracy: 0.4920 - type_sparse_categorical_accuracy: 0.5720 - val_loss: 21.4304 - val_price_loss: 28.5235 - val_type_loss: 14.3373 - val_price_sparse_categorical_accuracy: 0.3221 - val_type_sparse_categorical_accuracy: 0.6692\n",
      "Epoch 2/5\n",
      "234/234 [==============================] - 292s 1s/step - loss: 12.7582 - price_loss: 10.6210 - type_loss: 14.8953 - price_sparse_categorical_accuracy: 0.5142 - type_sparse_categorical_accuracy: 0.5779 - val_loss: 17.5407 - val_price_loss: 22.4734 - val_type_loss: 12.6080 - val_price_sparse_categorical_accuracy: 0.1155 - val_type_sparse_categorical_accuracy: 0.7628\n",
      "Epoch 3/5\n",
      "234/234 [==============================] - 298s 1s/step - loss: 7.8175 - price_loss: 5.9612 - type_loss: 9.6738 - price_sparse_categorical_accuracy: 0.5423 - type_sparse_categorical_accuracy: 0.5851 - val_loss: 9.4962 - val_price_loss: 5.6585 - val_type_loss: 13.3339 - val_price_sparse_categorical_accuracy: 0.5587 - val_type_sparse_categorical_accuracy: 0.3845\n",
      "Epoch 4/5\n",
      "234/234 [==============================] - 261s 1s/step - loss: 5.6272 - price_loss: 4.4690 - type_loss: 6.7853 - price_sparse_categorical_accuracy: 0.5335 - type_sparse_categorical_accuracy: 0.5926 - val_loss: 7.8455 - val_price_loss: 5.4928 - val_type_loss: 10.1983 - val_price_sparse_categorical_accuracy: 0.4806 - val_type_sparse_categorical_accuracy: 0.4863\n",
      "Epoch 5/5\n",
      "234/234 [==============================] - 256s 1s/step - loss: 5.3873 - price_loss: 4.3744 - type_loss: 6.4003 - price_sparse_categorical_accuracy: 0.5423 - type_sparse_categorical_accuracy: 0.5956 - val_loss: 4.6795 - val_price_loss: 3.7470 - val_type_loss: 5.6121 - val_price_sparse_categorical_accuracy: 0.5387 - val_type_sparse_categorical_accuracy: 0.6916\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( #fiting model\n",
    "    x={ #inputs\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={ #outputs\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=5, #num of ephocs ( number of passes through the training set) \n",
    "    batch_size=16, #numb of samples that will pass to network at a time\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
    "    ],\n",
    "    verbose=1 #shows progress and a line per each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed2d42d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef79bbf7d3f74640b644d0bac6c66539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_summary = _preprocess(ts.summary.astype(str)) \n",
    "x_test_image = np.array([load_image(i) for i in tqdm(ts.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426921f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99991536e-01 8.18381613e-06 2.66567611e-07]\n",
      " [9.99988914e-01 1.09100247e-05 1.53425063e-07]\n",
      " [8.97141039e-01 6.78245574e-02 3.50343995e-02]\n",
      " ...\n",
      " [9.93793249e-01 1.24735625e-05 6.19432237e-03]\n",
      " [6.35580858e-03 1.38107187e-03 9.92263079e-01]\n",
      " [9.98858094e-01 1.11520337e-03 2.67184005e-05]]\n",
      "[0 0 0 ... 0 2 0]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict( #predicting results\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "price_predicted = y_predict['price']\n",
    "print(price_predicted) #predicting price\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame( #generating submission file\n",
    "    {'id': ts.index,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774de56d",
   "metadata": {},
   "source": [
    "# Trial two\n",
    "\n",
    "using LSTM layer for text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057968f",
   "metadata": {},
   "source": [
    "# Expectaion\n",
    "\n",
    "I expect to get a high accuracy on kaggle after using LSTM layer for text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534fa67",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "After running my code, my accuracy on kaggle dropped to 0.43369 :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0bbc9d",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "I will use a BiDirectional layer for text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfadf5",
   "metadata": {},
   "source": [
    "#  BiDirectional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c691a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 100, 100)     4000000     ['input_7[0][0]']                \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, 64, 64, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 100, 200)     160800      ['embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 49, 49, 32)   16416       ['input_8[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 100, 200)    240800      ['bidirectional[0][0]']          \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 32)    0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_2 (Bidirectional  (None, 200)         240800      ['bidirectional_1[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)            (None, 288)          0           ['max_pooling2d_4[0][0]']        \n",
      "                                                                                                  \n",
      " tf.concat_3 (TFOpLambda)       (None, 488)          0           ['bidirectional_2[0][0]',        \n",
      "                                                                  'flatten_4[0][0]']              \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            1467        ['tf.concat_3[0][0]']            \n",
      "                                                                                                  \n",
      " type (Dense)                   (None, 24)           11736       ['tf.concat_3[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,672,019\n",
      "Trainable params: 4,672,019\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# image part\n",
    "# simple conv2d\n",
    "# first convolution layer\n",
    "# the first parameter is the filter indicating the dimensionality of the output space.\n",
    "# second oe is the kernel_size it tells the height and width of the filter.\n",
    "cov = Conv2D(32, (16, 16))(in_image)\n",
    "pl = MaxPool2D((16, 16))(cov)\n",
    "flattened = Flatten()(pl)\n",
    "\n",
    "\n",
    "# fusion - combining both:\n",
    "# concatenates tensors along one dimension (axis)\n",
    "# Bidirectional RNN\n",
    "# Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together.\n",
    "# This structure allows the networks to have both backward and forward\n",
    "# information about the sequence at every time step.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "in_text = keras.Input(batch_shape=(None, max_len))\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
    "\n",
    "# Bidirectional Layer:\n",
    "# Bidirectional LSTMs are an extension of traditional LSTMs that can improve model\n",
    "# performance on sequence classification problems.\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "bidirectional = keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True))(embedded)\n",
    "bidirectional1 = keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True))(bidirectional)\n",
    "bidirectional2 = keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=False))(bidirectional1)\n",
    "\n",
    "# image part\n",
    "cov = Conv2D(32, (16, 16))(in_image)\n",
    "pl = MaxPool2D((16, 16))(cov)\n",
    "flattened = Flatten()(pl)\n",
    "\n",
    "# fusion:\n",
    "fused = tf.concat([bidirectional2, flattened], axis=-1)\n",
    "\n",
    "# multi-task learning (each is a multi-class classification)\n",
    "# one dense layer for each task\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
    "\n",
    "# define model input/output using keys.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': in_text,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={ #to be predicted\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "# compile model with optimizer, loss values for each task, loss \n",
    "# weights for each task.\n",
    "from tensorflow.keras.optimizers import Adam #optimizer\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(), # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca191338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "234/234 [==============================] - 347s 1s/step - loss: 29.2694 - price_loss: 24.9294 - type_loss: 33.6094 - price_sparse_categorical_accuracy: 0.5059 - type_sparse_categorical_accuracy: 0.5715 - val_loss: 11.9814 - val_price_loss: 7.2505 - val_type_loss: 16.7123 - val_price_sparse_categorical_accuracy: 0.5362 - val_type_sparse_categorical_accuracy: 0.1654\n",
      "Epoch 2/5\n",
      "234/234 [==============================] - 329s 1s/step - loss: 8.1705 - price_loss: 6.3059 - type_loss: 10.0352 - price_sparse_categorical_accuracy: 0.5278 - type_sparse_categorical_accuracy: 0.5779 - val_loss: 6.2277 - val_price_loss: 4.4000 - val_type_loss: 8.0554 - val_price_sparse_categorical_accuracy: 0.5968 - val_type_sparse_categorical_accuracy: 0.6848\n",
      "Epoch 3/5\n",
      "234/234 [==============================] - 333s 1s/step - loss: 10.2234 - price_loss: 6.7054 - type_loss: 13.7413 - price_sparse_categorical_accuracy: 0.5476 - type_sparse_categorical_accuracy: 0.5840 - val_loss: 7.3341 - val_price_loss: 5.4166 - val_type_loss: 9.2516 - val_price_sparse_categorical_accuracy: 0.5930 - val_type_sparse_categorical_accuracy: 0.7347\n",
      "Epoch 4/5\n",
      "234/234 [==============================] - 347s 1s/step - loss: 6.6254 - price_loss: 4.4693 - type_loss: 8.7815 - price_sparse_categorical_accuracy: 0.5656 - type_sparse_categorical_accuracy: 0.5865 - val_loss: 5.0277 - val_price_loss: 3.8062 - val_type_loss: 6.2492 - val_price_sparse_categorical_accuracy: 0.5493 - val_type_sparse_categorical_accuracy: 0.3527\n",
      "Epoch 5/5\n",
      "234/234 [==============================] - 1851s 8s/step - loss: 6.1288 - price_loss: 4.1705 - type_loss: 8.0872 - price_sparse_categorical_accuracy: 0.5302 - type_sparse_categorical_accuracy: 0.5889 - val_loss: 5.4743 - val_price_loss: 5.0329 - val_type_loss: 5.9156 - val_price_sparse_categorical_accuracy: 0.5787 - val_type_sparse_categorical_accuracy: 0.5056\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( #fiting model\n",
    "    x={ #inputs\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={ #outputs\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=5, #num of ephocs ( number of passes through the training set) \n",
    "    batch_size=16, #numb of samples that will pass to network at a time\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
    "    ],\n",
    "    verbose=1 #shows progress and a line per each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bdae3be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc6792414c5c4b26ac073248d53a329c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_summary = _preprocess(ts.summary.astype(str)) \n",
    "x_test_image = np.array([load_image(i) for i in tqdm(ts.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14048431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 5.2564827e-11 1.4762740e-10]\n",
      " [9.9996305e-01 3.6965408e-05 4.5654102e-11]\n",
      " [1.0000000e+00 4.4944084e-08 9.0852923e-16]\n",
      " ...\n",
      " [1.2692772e-02 9.8730719e-01 7.8246767e-12]\n",
      " [4.0130215e-03 9.9598700e-01 5.8214953e-17]\n",
      " [9.9810398e-01 1.8960247e-03 1.7611444e-11]]\n",
      "[0 0 0 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict( #predicting results\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "price_predicted = y_predict['price']\n",
    "print(price_predicted) #predicting price\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame( #generating submission file\n",
    "    {'id': ts.index,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d66f4d",
   "metadata": {},
   "source": [
    "# Trial three\n",
    "\n",
    "using BiDirectional layer for text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cdab95",
   "metadata": {},
   "source": [
    "# Expectaion\n",
    "\n",
    "I expect to get a high accuracy on kaggle after using BiDirectional layer for text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbb55a5",
   "metadata": {},
   "source": [
    "# observation \n",
    "\n",
    "After running my code, my accuracy on kaggle increased to 0.48423"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbe410c",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "I will use BiDirectional layer for text inputs and Dropout layer for image inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986d88e",
   "metadata": {},
   "source": [
    "# BiDirectional layer for text inputs and Dropout layer for image inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40511038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fusion - combining both:\n",
    "# concatenates tensors along one dimension (axis)\n",
    "# Bidirectional RNN\n",
    "# Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together.\n",
    "# This structure allows the networks to have both backward and forward\n",
    "# information about the sequence at every time step.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "in_text = keras.Input(batch_shape=(None, max_len))\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96c169d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " embedding_4 (Embedding)        (None, 100, 100)     4000000     ['input_9[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 63, 63, 45)   405         ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_3 (Bidirectional  (None, 100, 200)    160800      ['embedding_4[0][0]']            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 63, 63, 45)   0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " bidirectional_4 (Bidirectional  (None, 100, 200)    240800      ['bidirectional_3[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 21, 21, 45)  0           ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional_5 (Bidirectional  (None, 200)         240800      ['bidirectional_4[0][0]']        \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 19845)        0           ['max_pooling2d_5[0][0]']        \n",
      "                                                                                                  \n",
      " tf.concat_4 (TFOpLambda)       (None, 20045)        0           ['bidirectional_5[0][0]',        \n",
      "                                                                  'flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            60138       ['tf.concat_4[0][0]']            \n",
      "                                                                                                  \n",
      " type (Dense)                   (None, 24)           481104      ['tf.concat_4[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 5,184,047\n",
      "Trainable params: 5,184,047\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional Layer:\n",
    "# Bidirectional LSTMs are an extension of traditional LSTMs that can improve model\n",
    "# performance on sequence classification problems.\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "bidirectional = keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True))(embedded)\n",
    "bidirectional1 = keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=True))(bidirectional)\n",
    "bidirectional2 = keras.layers.Bidirectional(keras.layers.LSTM(100, return_sequences=False))(bidirectional1)\n",
    "\n",
    "# image part\n",
    "drop_1 = tf.keras.layers.Dropout(.1) #dropout layer specification\n",
    "\n",
    "cov1 = Conv2D(45, (2, 2))(in_image) #2D Convolutional layer with 32 filters and filter size of 16x16 Output:(None, 63, 63, 45)\n",
    "outputs = drop_1(cov1, training=True) #(None, 63, 63, 45) \n",
    "p1 = MaxPool2D((3, 3))(outputs) #(None, 21, 21, 45)\n",
    "#cov2 = Conv2D(16, (2, 2))(p1)\n",
    "#p2 = MaxPool2D((2, 2))(cov2)        \n",
    "flattened = Flatten()(p1)            #flattening layer Output Shape (None, 19845) \n",
    "fc2 = tf.keras.layers.Dense(200, activation='relu')(flattened)\n",
    "\n",
    "# fusion:\n",
    "fused = tf.concat([bidirectional2, flattened], axis=-1)\n",
    "\n",
    "# multi-task learning (each is a multi-class classification)\n",
    "# one dense layer for each task\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
    "\n",
    "# define model input/output using keys.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': in_text,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={ #to be predicted\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "# compile model with optimizer, loss values for each task, loss \n",
    "# weights for each task.\n",
    "from tensorflow.keras.optimizers import Adam #optimizer\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(), # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "669407c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "234/234 [==============================] - 376s 1s/step - loss: 57.1222 - price_loss: 40.6415 - type_loss: 73.6030 - price_sparse_categorical_accuracy: 0.5037 - type_sparse_categorical_accuracy: 0.5691 - val_loss: 13.5224 - val_price_loss: 11.7410 - val_type_loss: 15.3038 - val_price_sparse_categorical_accuracy: 0.3964 - val_type_sparse_categorical_accuracy: 0.5044\n",
      "Epoch 2/5\n",
      "234/234 [==============================] - 295s 1s/step - loss: 8.7142 - price_loss: 7.4767 - type_loss: 9.9517 - price_sparse_categorical_accuracy: 0.5399 - type_sparse_categorical_accuracy: 0.6081 - val_loss: 9.7338 - val_price_loss: 7.4987 - val_type_loss: 11.9689 - val_price_sparse_categorical_accuracy: 0.4875 - val_type_sparse_categorical_accuracy: 0.6042\n",
      "Epoch 3/5\n",
      "234/234 [==============================] - 296s 1s/step - loss: 5.1792 - price_loss: 4.9085 - type_loss: 5.4500 - price_sparse_categorical_accuracy: 0.5806 - type_sparse_categorical_accuracy: 0.6555 - val_loss: 7.9155 - val_price_loss: 6.5657 - val_type_loss: 9.2653 - val_price_sparse_categorical_accuracy: 0.4632 - val_type_sparse_categorical_accuracy: 0.6361\n",
      "Epoch 4/5\n",
      "234/234 [==============================] - 342s 1s/step - loss: 3.5859 - price_loss: 3.7734 - type_loss: 3.3984 - price_sparse_categorical_accuracy: 0.6105 - type_sparse_categorical_accuracy: 0.6994 - val_loss: 7.1208 - val_price_loss: 5.4375 - val_type_loss: 8.8042 - val_price_sparse_categorical_accuracy: 0.4419 - val_type_sparse_categorical_accuracy: 0.6042\n",
      "Epoch 5/5\n",
      "234/234 [==============================] - 335s 1s/step - loss: 2.7158 - price_loss: 2.8837 - type_loss: 2.5479 - price_sparse_categorical_accuracy: 0.6392 - type_sparse_categorical_accuracy: 0.7452 - val_loss: 8.0778 - val_price_loss: 7.0593 - val_type_loss: 9.0963 - val_price_sparse_categorical_accuracy: 0.4257 - val_type_sparse_categorical_accuracy: 0.6792\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( #fiting model\n",
    "    x={ #inputs\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={ #outputs\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=5, #num of ephocs ( number of passes through the training set) \n",
    "    batch_size=16, #numb of samples that will pass to network at a time\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
    "    ],\n",
    "    verbose=1 #shows progress and a line per each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df5aec4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b978323212348308475dfb641aafd2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_summary = _preprocess(ts.summary.astype(str)) \n",
    "x_test_image = np.array([load_image(i) for i in tqdm(ts.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed5cb857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5635708e-01 8.4364289e-01 7.8341578e-15]\n",
      " [4.7073895e-01 5.2926099e-01 1.2623823e-11]\n",
      " [9.9974149e-01 2.5852400e-04 1.8917454e-11]\n",
      " ...\n",
      " [4.3199950e-09 1.0000000e+00 6.7965083e-14]\n",
      " [9.9998224e-01 1.7753038e-05 2.6690757e-16]\n",
      " [3.1571828e-03 9.9684227e-01 6.5280989e-07]]\n",
      "[1 1 0 ... 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict( #predicting results\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "price_predicted = y_predict['price']\n",
    "print(price_predicted) #predicting price\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame( #generating submission file\n",
    "    {'id': ts.index,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4670a2",
   "metadata": {},
   "source": [
    "# Trial four\n",
    "\n",
    "I tried using BiDirectional layer for text inputs and Dropout layer for image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8116630",
   "metadata": {},
   "source": [
    "# Expectation\n",
    "\n",
    "I expected to get a high kaggle accuracy after using BiDirectional layer for text inputs and Dropout layer for image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d9b07",
   "metadata": {},
   "source": [
    "# observation \n",
    "\n",
    "My kaggle accuracy increased to 0.55543"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663102f",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "I will use LSTM layer for text inputs & dropout layer for image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e195a",
   "metadata": {},
   "source": [
    "# LSTM layer for text inputs & dropout layer for image inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "05af5015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " embedding_5 (Embedding)        (None, 100, 100)     4000000     ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 63, 63, 45)   405         ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_12 (LSTM)                 (None, 100, 124)     111600      ['embedding_5[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 63, 63, 45)   0           ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " lstm_13 (LSTM)                 (None, 100, 64)      48384       ['lstm_12[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 21, 21, 45)  0           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " lstm_14 (LSTM)                 (None, 32)           12416       ['lstm_13[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 19845)        0           ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " tf.concat_5 (TFOpLambda)       (None, 19877)        0           ['lstm_14[0][0]',                \n",
      "                                                                  'flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            59634       ['tf.concat_5[0][0]']            \n",
      "                                                                                                  \n",
      " type (Dense)                   (None, 24)           477072      ['tf.concat_5[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,709,511\n",
      "Trainable params: 4,709,511\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Long Short Term Memory (LSTM)\n",
    "# Long Short Term Memory networks – usually just called “LSTMs”\n",
    "# It can handle not only individual data points like photos, but also speech or video\n",
    "\n",
    "from tensorflow import keras #Keras is a library that will help us make neural networks.\n",
    "import tensorflow as tf #open-source library for numerical computation\n",
    "\n",
    "\n",
    "# defines an input layer ,Input() is used to instantiate a Keras tensor, (instantiate a Keras tensor object) and allows for building a model\n",
    "in_text = keras.Input(batch_shape=(None, max_len))\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
    "\n",
    "# activation determines the output of that node given an input or group of inputs.\n",
    "# recurrent_activation: Activation function to use for the recurrent step\n",
    "# dropout: to avoid overfitting in training\n",
    "# rcurrent_dropout: regularization method for recurrent neural networks.\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "lstm = keras.layers.LSTM(124, \n",
    "                         activation=\"tanh\",\n",
    "                         recurrent_activation=\"sigmoid\",\n",
    "                         return_sequences=True,\n",
    "                         dropout=0.1,\n",
    "                         recurrent_dropout=0.1)(embedded)\n",
    "lstm2 = keras.layers.LSTM(64, \n",
    "                         activation=\"tanh\",\n",
    "                         recurrent_activation=\"sigmoid\",\n",
    "                         return_sequences=True,\n",
    "                         dropout=0.1,\n",
    "                         recurrent_dropout=0.1)(lstm)\n",
    "lstm3 = keras.layers.LSTM(32, \n",
    "                         activation=\"tanh\",\n",
    "                         recurrent_activation=\"sigmoid\",\n",
    "                         return_sequences=False,\n",
    "                         dropout=0.1,\n",
    "                         recurrent_dropout=0.1)(lstm2)\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "\n",
    "\n",
    "\n",
    "# image part\n",
    "drop_1 = tf.keras.layers.Dropout(.1) #dropout layer specification\n",
    "\n",
    "cov1 = Conv2D(45, (2, 2))(in_image) #2D Convolutional layer with 32 filters and filter size of 16x16 Output:(None, 63, 63, 45)\n",
    "outputs = drop_1(cov1, training=True) #(None, 63, 63, 45) \n",
    "p1 = MaxPool2D((3, 3))(outputs) #(None, 21, 21, 45)\n",
    "#cov2 = Conv2D(16, (2, 2))(p1)\n",
    "#p2 = MaxPool2D((2, 2))(cov2)        \n",
    "flattened = Flatten()(p1)            #flattening layer Output Shape (None, 19845) \n",
    "fc2 = tf.keras.layers.Dense(200, activation='relu')(flattened)\n",
    "\n",
    "\n",
    "\n",
    "# fusion - combining both:\n",
    "# concatenates tensors along one dimension (axis)\n",
    "fused = tf.concat([lstm3, flattened], axis=-1)\n",
    "\n",
    "# multi-task learning (each is a multi-class classification)\n",
    "# one dense layer for each task\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
    "\n",
    "# define model input/output using keys.\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': in_text,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={ #to be predicted\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "# compile model with optimizer, loss values for each task, loss \n",
    "# weights for each task.\n",
    "from tensorflow.keras.optimizers import Adam #optimizer\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(), # Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dc0ea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "234/234 [==============================] - 439s 1s/step - loss: 45.6848 - price_loss: 36.4765 - type_loss: 54.8931 - price_sparse_categorical_accuracy: 0.5040 - type_sparse_categorical_accuracy: 0.5728 - val_loss: 11.9748 - val_price_loss: 10.6857 - val_type_loss: 13.2640 - val_price_sparse_categorical_accuracy: 0.5031 - val_type_sparse_categorical_accuracy: 0.4969\n",
      "Epoch 2/5\n",
      "234/234 [==============================] - 286s 1s/step - loss: 8.4095 - price_loss: 7.0200 - type_loss: 9.7990 - price_sparse_categorical_accuracy: 0.5522 - type_sparse_categorical_accuracy: 0.6004 - val_loss: 8.4847 - val_price_loss: 6.7977 - val_type_loss: 10.1717 - val_price_sparse_categorical_accuracy: 0.4419 - val_type_sparse_categorical_accuracy: 0.4732\n",
      "Epoch 3/5\n",
      "234/234 [==============================] - 289s 1s/step - loss: 5.2012 - price_loss: 4.9381 - type_loss: 5.4643 - price_sparse_categorical_accuracy: 0.5720 - type_sparse_categorical_accuracy: 0.6448 - val_loss: 7.1095 - val_price_loss: 6.0411 - val_type_loss: 8.1779 - val_price_sparse_categorical_accuracy: 0.5312 - val_type_sparse_categorical_accuracy: 0.5531\n",
      "Epoch 4/5\n",
      "234/234 [==============================] - 15889s 68s/step - loss: 3.5215 - price_loss: 3.4955 - type_loss: 3.5474 - price_sparse_categorical_accuracy: 0.6237 - type_sparse_categorical_accuracy: 0.7002 - val_loss: 6.7182 - val_price_loss: 6.0227 - val_type_loss: 7.4138 - val_price_sparse_categorical_accuracy: 0.4413 - val_type_sparse_categorical_accuracy: 0.5749\n",
      "Epoch 5/5\n",
      "234/234 [==============================] - 284s 1s/step - loss: 2.8054 - price_loss: 2.9966 - type_loss: 2.6142 - price_sparse_categorical_accuracy: 0.6317 - type_sparse_categorical_accuracy: 0.7409 - val_loss: 6.4195 - val_price_loss: 5.7493 - val_type_loss: 7.0897 - val_price_sparse_categorical_accuracy: 0.5449 - val_type_sparse_categorical_accuracy: 0.6348\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( #fiting model\n",
    "    x={ #inputs\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={ #outputs\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=5, #num of ephocs ( number of passes through the training set) \n",
    "    batch_size=16, #numb of samples that will pass to network at a time\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
    "    ],\n",
    "    verbose=1 #shows progress and a line per each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8185c378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e6d4c05a6864caeb99a0c2967e42dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_summary = _preprocess(ts.summary.astype(str)) \n",
    "x_test_image = np.array([load_image(i) for i in tqdm(ts.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31a131dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.00000000e+00 1.52129420e-11 3.23245255e-14]\n",
      " [1.00000000e+00 3.67266786e-08 8.80627544e-15]\n",
      " [9.99992251e-01 5.62850211e-09 7.72380736e-06]\n",
      " ...\n",
      " [1.00000000e+00 3.46860562e-09 1.79525549e-16]\n",
      " [9.99987245e-01 1.27010735e-05 9.14231624e-12]\n",
      " [9.99929428e-01 6.02013170e-05 1.04160254e-05]]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict( #predicting results\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "price_predicted = y_predict['price']\n",
    "print(price_predicted) #predicting price\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame( #generating submission file\n",
    "    {'id': ts.index,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7f1980",
   "metadata": {},
   "source": [
    "# Trial five \n",
    "\n",
    "I tried using # LSTM layer for text inputs & dropout layer for image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64be5d08",
   "metadata": {},
   "source": [
    "# Expectation\n",
    "\n",
    "I expected getting a high kaggle accuracy after using LSTM layer for text inputs & dropout layer for image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b73c091",
   "metadata": {},
   "source": [
    "# observation\n",
    "\n",
    "my accuracy dropped to 0.50489"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af1c143",
   "metadata": {},
   "source": [
    "# Plan\n",
    "\n",
    "I will use GRU layer for text input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0ac96c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, 100, 100)     4000000     ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " input_14 (InputLayer)          [(None, 64, 64, 2)]  0           []                               \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 100, 124)     84072       ['embedding_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 49, 49, 32)   16416       ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 100, 64)      36480       ['gru[0][0]']                    \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 3, 3, 32)    0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                    (None, 32)           9408        ['gru_1[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_7 (Flatten)            (None, 288)          0           ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " tf.concat_6 (TFOpLambda)       (None, 320)          0           ['gru_2[0][0]',                  \n",
      "                                                                  'flatten_7[0][0]']              \n",
      "                                                                                                  \n",
      " price (Dense)                  (None, 3)            963         ['tf.concat_6[0][0]']            \n",
      "                                                                                                  \n",
      " type (Dense)                   (None, 24)           7704        ['tf.concat_6[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,155,043\n",
      "Trainable params: 4,155,043\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Gated Recurrent Unit (GRU)\n",
    "# The structure of the GRU allows it to adaptively capture dependencies\n",
    "# from large sequences of data without discarding information from earlier\n",
    "# parts of the sequence. This is achieved through its gating units.\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "in_text = keras.Input(batch_shape=(None, max_len))\n",
    "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
    "\n",
    "# GRU paramerters:\n",
    "# units: Positive integer, dimensionality of the output space.\n",
    "# activation: Activation function to use. Default: hyperbolic tangent (tanh). If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "# recurrent_activation: Activation function to use for the recurrent step. Default: sigmoid (sigmoid). If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "# dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. Default: 0.\n",
    "# recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. Default: 0.\n",
    "# return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False.\n",
    "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
    "gru = keras.layers.GRU(124, \n",
    "                       activation=\"tanh\",\n",
    "                       recurrent_activation=\"sigmoid\",\n",
    "                       return_sequences=True,\n",
    "                       dropout=0.1,\n",
    "                       recurrent_dropout=0.1)(embedded)\n",
    "gru2 = keras.layers.GRU(64, \n",
    "                        activation=\"tanh\",\n",
    "                        recurrent_activation=\"sigmoid\",\n",
    "                        return_sequences=True,\n",
    "                        dropout=0.1,\n",
    "                        recurrent_dropout=0.1)(gru)\n",
    "gru3 = keras.layers.GRU(32, \n",
    "                        activation=\"tanh\",\n",
    "                        recurrent_activation=\"sigmoid\",\n",
    "                        return_sequences=False,\n",
    "                        dropout=0.1,\n",
    "                        recurrent_dropout=0.1)(gru2)\n",
    "\n",
    "# image part\n",
    "cov = Conv2D(32, (16, 16))(in_image)\n",
    "pl = MaxPool2D((16, 16))(cov)\n",
    "flattened = Flatten()(pl)\n",
    "\n",
    "# fusion:\n",
    "fused = tf.concat([gru3, flattened], axis=-1)\n",
    "\n",
    "# multi-objectives (each is a multi-class classification)\n",
    "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
    "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
    "\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs={\n",
    "        'summary': in_text,\n",
    "        'image': in_image\n",
    "    },\n",
    "    outputs={\n",
    "        'price': p_price,\n",
    "        'type': p_type,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss={\n",
    "        'price': 'sparse_categorical_crossentropy',\n",
    "        'type': 'sparse_categorical_crossentropy',\n",
    "    },\n",
    "    loss_weights={\n",
    "        'price': 0.5,\n",
    "        'type': 0.5,       \n",
    "    },\n",
    "    metrics={\n",
    "        'price': ['SparseCategoricalAccuracy'],\n",
    "        'type': ['SparseCategoricalAccuracy'],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a312cec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "234/234 [==============================] - 346s 1s/step - loss: 27.7542 - price_loss: 23.7706 - type_loss: 31.7379 - price_sparse_categorical_accuracy: 0.4957 - type_sparse_categorical_accuracy: 0.5650 - val_loss: 10.0212 - val_price_loss: 6.4730 - val_type_loss: 13.5695 - val_price_sparse_categorical_accuracy: 0.5593 - val_type_sparse_categorical_accuracy: 0.6242\n",
      "Epoch 2/5\n",
      "234/234 [==============================] - 275s 1s/step - loss: 10.9991 - price_loss: 8.0523 - type_loss: 13.9459 - price_sparse_categorical_accuracy: 0.5150 - type_sparse_categorical_accuracy: 0.5715 - val_loss: 7.3369 - val_price_loss: 4.9883 - val_type_loss: 9.6855 - val_price_sparse_categorical_accuracy: 0.5337 - val_type_sparse_categorical_accuracy: 0.6542\n",
      "Epoch 3/5\n",
      "234/234 [==============================] - 275s 1s/step - loss: 9.5719 - price_loss: 7.1899 - type_loss: 11.9539 - price_sparse_categorical_accuracy: 0.5134 - type_sparse_categorical_accuracy: 0.5819 - val_loss: 8.1958 - val_price_loss: 6.7845 - val_type_loss: 9.6071 - val_price_sparse_categorical_accuracy: 0.5137 - val_type_sparse_categorical_accuracy: 0.7459\n",
      "Epoch 4/5\n",
      "234/234 [==============================] - 290s 1s/step - loss: 6.4269 - price_loss: 4.9195 - type_loss: 7.9344 - price_sparse_categorical_accuracy: 0.5410 - type_sparse_categorical_accuracy: 0.5921 - val_loss: 7.3284 - val_price_loss: 4.9933 - val_type_loss: 9.6635 - val_price_sparse_categorical_accuracy: 0.4288 - val_type_sparse_categorical_accuracy: 0.5811\n",
      "Epoch 5/5\n",
      "234/234 [==============================] - 299s 1s/step - loss: 6.1131 - price_loss: 4.6981 - type_loss: 7.5281 - price_sparse_categorical_accuracy: 0.5345 - type_sparse_categorical_accuracy: 0.5913 - val_loss: 6.4776 - val_price_loss: 6.6140 - val_type_loss: 6.3412 - val_price_sparse_categorical_accuracy: 0.6017 - val_type_sparse_categorical_accuracy: 0.6554\n"
     ]
    }
   ],
   "source": [
    "history = model.fit( #fiting model\n",
    "    x={ #inputs\n",
    "        'summary': x_tr_text_id,\n",
    "        'image': x_tr_image\n",
    "    },\n",
    "    y={ #outputs\n",
    "        'price': y_tr_price,\n",
    "        'type': y_tr_type,\n",
    "    },\n",
    "    epochs=5, #num of ephocs ( number of passes through the training set) \n",
    "    batch_size=16, #numb of samples that will pass to network at a time\n",
    "    validation_data=(\n",
    "        {\n",
    "            'summary': x_vl_text_id,\n",
    "            'image': x_vl_image\n",
    "         }, \n",
    "        {\n",
    "            'price': y_vl_price,\n",
    "            'type': y_vl_type,\n",
    "        }),\n",
    "    validation_split=0.3,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_price_loss', patience=5, )\n",
    "    ],\n",
    "    verbose=1 #shows progress and a line per each epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c457e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7583e4e6172748288cc915566357a520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_test_summary = _preprocess(ts.summary.astype(str)) \n",
    "x_test_image = np.array([load_image(i) for i in tqdm(ts.image)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "973fc4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0000000e+00 4.4918269e-09 5.1939990e-13]\n",
      " [3.6302037e-04 9.9963701e-01 2.3723864e-08]\n",
      " [1.0000000e+00 6.0809198e-09 7.1697899e-13]\n",
      " ...\n",
      " [1.0000000e+00 4.5051149e-10 6.6938327e-10]\n",
      " [9.9561018e-01 4.3898080e-03 5.1747978e-10]\n",
      " [9.9998283e-01 1.3732498e-05 3.4867060e-06]]\n",
      "[0 1 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "y_predict = model.predict( #predicting results\n",
    "    {\n",
    "        'summary': x_test_summary,\n",
    "        'image': x_test_image\n",
    "    }\n",
    ")\n",
    "\n",
    "price_predicted = y_predict['price']\n",
    "print(price_predicted) #predicting price\n",
    "price_category_predicted = np.argmax(price_predicted, axis=1)\n",
    "print(price_category_predicted)\n",
    "\n",
    "pd.DataFrame( #generating submission file\n",
    "    {'id': ts.index,\n",
    "     'price': price_category_predicted}).to_csv('sample_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df202d5c",
   "metadata": {},
   "source": [
    "# Trial six\n",
    "\n",
    "I tried using GRU Layer for text inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eccf1d",
   "metadata": {},
   "source": [
    "# Expectation\n",
    "\n",
    "I expected getting a higher accuracy on kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd22a9",
   "metadata": {},
   "source": [
    "# Observation\n",
    "\n",
    "My kaggle accuracy dropped again to 0.38369"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd2e7c",
   "metadata": {},
   "source": [
    "# Resouces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706d780a",
   "metadata": {},
   "source": [
    "https://pypi.org/project/tqdm/\n",
    "\n",
    "https://www.aipython.in/python-literal_eval/\n",
    "\n",
    "https://keras.io/api/optimizers/adam/\n",
    "\n",
    "https://www.bing.com/search?q=What+is+the+search+space+in+deep+learning&go=Search&qs=ds&form=QBRE\n",
    "\n",
    "https://stats.stackexchange.com/questions/95495/guideline-to-select-the-hyperparameters-in-deep-learning\n",
    "\n",
    "https://www.oreilly.com/library/view/tensorflow-for-deep/9781491980446/ch04.html\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/\n",
    "\n",
    "https://towardsdatascience.com/the-vanishing-exploding-gradient-problem-in-deep-neural-networks-191358470c11\n",
    "\n",
    "https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n",
    "\n",
    "https://www.springboard.com/blog/data-science/xgboost-random-forest-catboost-lightgbm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0675c0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
